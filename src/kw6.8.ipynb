{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations --user > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pretrainedmodels --user > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mplimg\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time \n",
    "import tqdm\n",
    "from PIL import Image\n",
    "train_on_gpu = True\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from collections import OrderedDict\n",
    "import cv2\n",
    "import albumentations\n",
    "from albumentations import torch as AT\n",
    "import pretrainedmodels\n",
    "import csv\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b4284d5ef.jpg</td>\n",
       "      <td>w_16b5050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4b82a9fc0.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16bf136fb.jpg</td>\n",
       "      <td>w_e73cce3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9805abc71.jpg</td>\n",
       "      <td>w_d6fde02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4e2803603.jpg</td>\n",
       "      <td>w_83a4279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Image         Id\n",
       "0  b4284d5ef.jpg  w_16b5050\n",
       "1  4b82a9fc0.jpg  new_whale\n",
       "2  16bf136fb.jpg  w_e73cce3\n",
       "3  9805abc71.jpg  w_d6fde02\n",
       "4  4e2803603.jpg  w_83a4279"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/new_whale_test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20288 images in train dataset with 4571 unique classes.\n",
      "There are 5074 images in test dataset with 1522 unique classes.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(os.listdir('../data/train'))} images in train dataset with {train_df.Id.nunique()} unique classes.\")\n",
    "print(f\"There are {len(os.listdir('../data/test'))} images in test dataset with {test_df.Id.nunique()} unique classes.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_whale    7728\n",
       "w_23a388d      60\n",
       "w_9b5109b      55\n",
       "w_0369a5c      54\n",
       "w_9c506f6      52\n",
       "Name: Id, dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Id.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2182 classes with 1 samples in train data.\n",
      "There are 1085 classes with 2 samples in train data.\n",
      "There are 441 classes with 3 samples in train data.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    print(f'There are {train_df.Id.value_counts()[train_df.Id.value_counts().values==i].shape[0]} classes with {i} samples in train data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20288"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.Id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.Id.value_counts()[1:].plot(kind='hist');\n",
    "plt.title('Distribution of classes excluding new_whale');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(y):\n",
    "    # From here: https://www.kaggle.com/pestipeti/keras-cnn-starter\n",
    "    values = np.array(y)\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "    y = onehot_encoded\n",
    "    return y, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "y, le = prepare_labels(train_df['Id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhaleDataset(Dataset):\n",
    "    def __init__(self, datafolder, datatype='train', df=None, transform = transforms.Compose([transforms.ToTensor()]), y=None):\n",
    "        self.datafolder = datafolder\n",
    "        self.datatype = datatype\n",
    "        self.y = y\n",
    "        if self.datatype == 'train':\n",
    "            self.df = df.values\n",
    "        self.image_files_list = [s for s in os.listdir(datafolder)]\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.datatype == 'train':\n",
    "            img_name = os.path.join(self.datafolder, self.df[idx - 1][0])\n",
    "            label = self.y[idx - 1]\n",
    "            \n",
    "        elif self.datatype == 'test':\n",
    "            img_name = os.path.join(self.datafolder, self.image_files_list[idx - 1])\n",
    "            label = np.zeros((4571,))\n",
    "\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        if self.datatype == 'train':\n",
    "            return image, label\n",
    "        elif self.datatype == 'test':\n",
    "            # so that the images will be in a correct order\n",
    "            return image, label, self.image_files_list[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "                                      transforms.Resize((100, 100)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "data_transforms_test = transforms.Compose([\n",
    "                                           transforms.Resize((100, 100)),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                 std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WhaleDataset(datafolder='../data/train/', datatype='train', df=train_df, transform=data_transforms, y=y)\n",
    "test_set = WhaleDataset(datafolder='../data/test/', datatype='test', transform=data_transforms_test)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(list(range(len(os.listdir('../data/train')))))\n",
    "valid_sampler = SubsetRandomSampler(list(range(len(os.listdir('../data/test')))))\n",
    "batch_size = 128\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "# less size for test loader.\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 7, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)        \n",
    "        self.pool2 = nn.AvgPool2d(3, 3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 4 * 4 * 16, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4571) # todo\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv2_bn(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 4 * 4 * 16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = Net()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model_conv.parameters(), lr=0.01)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhaleDataset(Dataset):\n",
    "    def __init__(self, datafolder, datatype='train', df=None, transform = transforms.Compose([transforms.ToTensor()]), y=None):\n",
    "        self.datafolder = datafolder\n",
    "        self.datatype = datatype\n",
    "        self.y = y\n",
    "        if self.datatype == 'train':\n",
    "            self.df = df.values\n",
    "        self.image_files_list = [s for s in os.listdir(datafolder)]\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.datatype == 'train':\n",
    "            img_name = os.path.join(self.datafolder, self.df[idx - 1][0])\n",
    "            label = self.y[idx - 1]\n",
    "            \n",
    "        elif self.datatype == 'test':\n",
    "            img_name = os.path.join(self.datafolder, self.image_files_list[idx - 1])\n",
    "            label = np.zeros((4571,))\n",
    "\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        if self.datatype == 'train':\n",
    "            return image, label\n",
    "        elif self.datatype == 'test':\n",
    "            # so that the images will be in a correct order\n",
    "            return image, label, self.image_files_list[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "                                      transforms.RandomResizedCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "data_transforms_test = transforms.Compose([\n",
    "                                           transforms.RandomResizedCrop(224),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                 std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WhaleDataset(datafolder='../data/train/', datatype='train', df=train_df, transform=data_transforms, y=y)\n",
    "test_set = WhaleDataset(datafolder='../data/test/', datatype='test', transform=data_transforms_test)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(list(range(len(os.listdir('../data/train')))))\n",
    "valid_sampler = SubsetRandomSampler(list(range(len(os.listdir('../data/test')))))\n",
    "batch_size = 32\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "# less size for test loader.\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4571\n",
    "model_conv = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_conv.fc.in_features #最后fc层的输入\n",
    "model_conv.fc = nn.Linear(num_ftrs, num_classes) #NUM_CLASSES是自己数据的类别\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model_conv.parameters(), lr=0.01)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this before TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhaleDataset(Dataset):\n",
    "    def __init__(self, datafolder, datatype='train', df=None, transform = transforms.Compose([transforms.ToTensor()]), y=None):\n",
    "        self.datafolder = datafolder\n",
    "        self.datatype = datatype\n",
    "        self.y = y\n",
    "        if self.datatype == 'train':\n",
    "            self.df = df.values\n",
    "        self.image_files_list = [s for s in os.listdir(datafolder)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.datatype == 'train':\n",
    "            img_name = os.path.join(self.datafolder, self.df[idx - 1][0])\n",
    "            label = self.y[idx - 1]\n",
    "            \n",
    "        elif self.datatype == 'test':\n",
    "            img_name = os.path.join(self.datafolder, self.image_files_list[idx - 1])\n",
    "            label = np.zeros((4571,))\n",
    "\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transform(image=img)\n",
    "        image = image['image']\n",
    "        if self.datatype == 'train':\n",
    "            return image, label\n",
    "        elif self.datatype == 'test':\n",
    "            # so that the images will be in a correct order\n",
    "            return image, label, self.image_files_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = albumentations.Compose([\n",
    "    albumentations.Resize(160, 320),\n",
    "    albumentations.HorizontalFlip(),\n",
    "    albumentations.RandomBrightness(),\n",
    "    albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n",
    "    albumentations.JpegCompression(80),\n",
    "    albumentations.HueSaturationValue(),\n",
    "    albumentations.Normalize(),\n",
    "    AT.ToTensor()\n",
    "    ])\n",
    "data_transforms_test = albumentations.Compose([\n",
    "    albumentations.Resize(160, 320),\n",
    "    albumentations.Normalize(),\n",
    "    AT.ToTensor()\n",
    "    ])\n",
    "\n",
    "train_dataset = WhaleDataset(datafolder='../data/train/', datatype='train', df=train_df, transform=data_transforms, y=y)\n",
    "test_set = WhaleDataset(datafolder='../data/test/', datatype='test', transform=data_transforms_test)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(list(range(len(os.listdir('../data/train')))))\n",
    "valid_sampler = SubsetRandomSampler(list(range(len(os.listdir('../data/test')))))\n",
    "batch_size = 10\n",
    "num_workers = 0\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "#valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=10, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"http://data.lip6.fr/cadene/pretrainedmodels/resnext101_64x4d-e77a0586.pth\" to /datasets/home/48/248/k7wang/.cache/torch/checkpoints/resnext101_64x4d-e77a0586.pth\n",
      "  0%|          | 786432/334703243 [00:02<16:54, 329115.97it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-11b68c6ea309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrainedmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnext101_64x4d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAvgPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_linear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pretrainedmodels/models/resnext.py\u001b[0m in \u001b[0;36mresnext101_64x4d\u001b[0;34m(num_classes, pretrained)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_classes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;34m\"num_classes should be {}, but is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_classes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_zoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_space'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading: \"{}\" to {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mhash_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHASH_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0m_download_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/hub.py\u001b[0m in \u001b[0;36m_download_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv = pretrainedmodels.resnext101_64x4d()\n",
    "model_conv.avg_pool = nn.AvgPool2d((5,10))\n",
    "model_conv.last_linear = nn.Linear(model_conv.last_linear.in_features, 4571)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv.cuda()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model_conv.parameters(), lr=0.01)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every model use the same one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun  8 14:31:08 2019 Epoch: 1\n",
      "Epoch 1, train loss: 0.012598100320418488, acc : 0.35227720820189273\n",
      "Sat Jun  8 14:35:38 2019 Epoch: 2\n",
      "Epoch 2, train loss: 0.0020184835796859465, acc : 0.3665713722397476\n",
      "Sat Jun  8 14:40:10 2019 Epoch: 3\n",
      "Epoch 3, train loss: 0.0018078331797495015, acc : 0.371352523659306\n",
      "Sat Jun  8 14:44:42 2019 Epoch: 4\n",
      "Epoch 4, train loss: 0.0017724444777792636, acc : 0.3737430993690852\n",
      "Sat Jun  8 14:49:15 2019 Epoch: 5\n",
      "Epoch 5, train loss: 0.0017658993894667746, acc : 0.3751774447949527\n",
      "Sat Jun  8 14:53:45 2019 Epoch: 6\n",
      "Epoch 6, train loss: 0.001735927807255033, acc : 0.37613367507886436\n",
      "Sat Jun  8 14:58:20 2019 Epoch: 7\n",
      "Epoch 7, train loss: 0.0017306901655107173, acc : 0.3768166967102298\n",
      "Sat Jun  8 15:02:52 2019 Epoch: 8\n"
     ]
    }
   ],
   "source": [
    "model_conv.cuda()\n",
    "n_epochs = 10\n",
    "acc = [0] * (n_epochs + 1)\n",
    "total = list(0. for i in range(train_df.Id.nunique()))\n",
    "correct = list(0. for i in range(train_df.Id.nunique()))\n",
    "llloss = [0] * (n_epochs + 1)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    print(time.ctime(), 'Epoch:', epoch)\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    exp_lr_scheduler.step()\n",
    "    accuracy = []\n",
    "    \n",
    "    # train\n",
    "    for batch_i, (data, target) in enumerate(train_loader):\n",
    "        #print(batch_i)\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model_conv(data)\n",
    "        predicted = torch.argmax(output, 1)\n",
    "\n",
    "        res = (predicted == torch.argmax(target, 1)).squeeze()\n",
    "        \n",
    "        for label_idx in range(len(target)):\n",
    "            \n",
    "            label_single = torch.argmax(target, 1)[label_idx]\n",
    "            correct[label_single] += res[label_idx].item()\n",
    "            total[label_single] += 1\n",
    "        \n",
    "        loss = criterion(output, target.float())\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "          \n",
    "    llloss[epoch] = np.mean(train_loss)\n",
    "    acc[epoch] = sum(correct)/sum(total)\n",
    "    print(f'Epoch {epoch}, train loss: {llloss[epoch]}, acc : {acc[epoch]}')\n",
    "    \n",
    "acc_base_train = acc\n",
    "loss_base_train = llloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_name = 'Simple_cnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_conv.state_dict(), net_name + '.pth')\n",
    "#model_conv.load_state_dict(torch.load(net_name + '.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_base_train = acc\n",
    "loss_base_train = llloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch = range(1, 5)\n",
    "plt.plot(epoch, acc_base_train[1:5], label = 'Accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.title(net_name + ' ACC')\n",
    "plt.legend()\n",
    "plt.savefig(f'../result/{net_name}_ACC_{time.ctime()}.jpg')#,dpi = 900)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch, loss_base_train[1:5], label = 'Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title(net_name + ' LOSS')\n",
    "plt.legend()\n",
    "plt.savefig(f'../result/{net_name}_LOSS_{time.ctime()}.jpg')#,dpi = 900)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exctrating data in batch0\n",
      "exctrating data in batch1\n",
      "exctrating data in batch2\n",
      "exctrating data in batch3\n",
      "exctrating data in batch4\n",
      "exctrating data in batch5\n",
      "exctrating data in batch6\n",
      "exctrating data in batch7\n",
      "exctrating data in batch8\n",
      "exctrating data in batch9\n",
      "exctrating data in batch10\n",
      "exctrating data in batch11\n",
      "exctrating data in batch12\n",
      "exctrating data in batch13\n",
      "exctrating data in batch14\n",
      "exctrating data in batch15\n",
      "exctrating data in batch16\n",
      "exctrating data in batch17\n",
      "exctrating data in batch18\n",
      "exctrating data in batch19\n",
      "exctrating data in batch20\n",
      "exctrating data in batch21\n",
      "exctrating data in batch22\n",
      "exctrating data in batch23\n",
      "exctrating data in batch24\n",
      "exctrating data in batch25\n",
      "exctrating data in batch26\n",
      "exctrating data in batch27\n",
      "exctrating data in batch28\n",
      "exctrating data in batch29\n",
      "exctrating data in batch30\n",
      "exctrating data in batch31\n",
      "exctrating data in batch32\n",
      "exctrating data in batch33\n",
      "exctrating data in batch34\n",
      "exctrating data in batch35\n",
      "exctrating data in batch36\n",
      "exctrating data in batch37\n",
      "exctrating data in batch38\n",
      "exctrating data in batch39\n",
      "exctrating data in batch40\n",
      "exctrating data in batch41\n",
      "exctrating data in batch42\n",
      "exctrating data in batch43\n",
      "exctrating data in batch44\n",
      "exctrating data in batch45\n",
      "exctrating data in batch46\n",
      "exctrating data in batch47\n",
      "exctrating data in batch48\n",
      "exctrating data in batch49\n",
      "exctrating data in batch50\n",
      "exctrating data in batch51\n",
      "exctrating data in batch52\n",
      "exctrating data in batch53\n",
      "exctrating data in batch54\n",
      "exctrating data in batch55\n",
      "exctrating data in batch56\n",
      "exctrating data in batch57\n",
      "exctrating data in batch58\n",
      "exctrating data in batch59\n",
      "exctrating data in batch60\n",
      "exctrating data in batch61\n",
      "exctrating data in batch62\n",
      "exctrating data in batch63\n",
      "exctrating data in batch64\n",
      "exctrating data in batch65\n",
      "exctrating data in batch66\n",
      "exctrating data in batch67\n",
      "exctrating data in batch68\n",
      "exctrating data in batch69\n",
      "exctrating data in batch70\n",
      "exctrating data in batch71\n",
      "exctrating data in batch72\n",
      "exctrating data in batch73\n",
      "exctrating data in batch74\n",
      "exctrating data in batch75\n",
      "exctrating data in batch76\n",
      "exctrating data in batch77\n",
      "exctrating data in batch78\n",
      "exctrating data in batch79\n",
      "exctrating data in batch80\n",
      "exctrating data in batch81\n",
      "exctrating data in batch82\n",
      "exctrating data in batch83\n",
      "exctrating data in batch84\n",
      "exctrating data in batch85\n",
      "exctrating data in batch86\n",
      "exctrating data in batch87\n",
      "exctrating data in batch88\n",
      "exctrating data in batch89\n",
      "exctrating data in batch90\n",
      "exctrating data in batch91\n",
      "exctrating data in batch92\n",
      "exctrating data in batch93\n",
      "exctrating data in batch94\n",
      "exctrating data in batch95\n",
      "exctrating data in batch96\n",
      "exctrating data in batch97\n",
      "exctrating data in batch98\n",
      "exctrating data in batch99\n",
      "exctrating data in batch100\n",
      "exctrating data in batch101\n",
      "exctrating data in batch102\n",
      "exctrating data in batch103\n",
      "exctrating data in batch104\n",
      "exctrating data in batch105\n",
      "exctrating data in batch106\n",
      "exctrating data in batch107\n",
      "exctrating data in batch108\n",
      "exctrating data in batch109\n",
      "exctrating data in batch110\n",
      "exctrating data in batch111\n",
      "exctrating data in batch112\n",
      "exctrating data in batch113\n",
      "exctrating data in batch114\n",
      "exctrating data in batch115\n",
      "exctrating data in batch116\n",
      "exctrating data in batch117\n",
      "exctrating data in batch118\n",
      "exctrating data in batch119\n",
      "exctrating data in batch120\n",
      "exctrating data in batch121\n",
      "exctrating data in batch122\n",
      "exctrating data in batch123\n",
      "exctrating data in batch124\n",
      "exctrating data in batch125\n",
      "exctrating data in batch126\n",
      "exctrating data in batch127\n",
      "exctrating data in batch128\n",
      "exctrating data in batch129\n",
      "exctrating data in batch130\n",
      "exctrating data in batch131\n",
      "exctrating data in batch132\n",
      "exctrating data in batch133\n",
      "exctrating data in batch134\n",
      "exctrating data in batch135\n",
      "exctrating data in batch136\n",
      "exctrating data in batch137\n",
      "exctrating data in batch138\n",
      "exctrating data in batch139\n",
      "exctrating data in batch140\n",
      "exctrating data in batch141\n",
      "exctrating data in batch142\n",
      "exctrating data in batch143\n",
      "exctrating data in batch144\n",
      "exctrating data in batch145\n",
      "exctrating data in batch146\n",
      "exctrating data in batch147\n",
      "exctrating data in batch148\n",
      "exctrating data in batch149\n",
      "exctrating data in batch150\n",
      "exctrating data in batch151\n",
      "exctrating data in batch152\n",
      "exctrating data in batch153\n",
      "exctrating data in batch154\n",
      "exctrating data in batch155\n",
      "exctrating data in batch156\n",
      "exctrating data in batch157\n",
      "exctrating data in batch158\n"
     ]
    }
   ],
   "source": [
    "ext_1 = []\n",
    "ext_2 = []\n",
    "#for (data, target, name) in train_loader:\n",
    "for batch_i, (data, target) in enumerate(train_loader):\n",
    "    if batch_i == 2000:\n",
    "        break\n",
    "    print(f'exctrating data in batch{batch_i}')\n",
    "    data = data.cuda()\n",
    "    model_conv.cuda()\n",
    "    output = model_conv(data)\n",
    "    for i in range(len(output)):\n",
    "        ext_1.append((output[i].cpu().detach().numpy(),target[i]))\n",
    "        ext_2.append((data[i].cpu().detach().numpy(),target[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 4571 into shape (1522,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-14bd3ce4a243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mext_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0my_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ext_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 4571 into shape (1522,)"
     ]
    }
   ],
   "source": [
    "dim_output = 1522\n",
    "x = np.zeros((len(ext_1), dim_output))\n",
    "y_n = np.zeros(len(ext_1))\n",
    "for i in range(len(ext_1)):\n",
    "    x[i, :] = ext_1[i][0].reshape(1, dim_output)\n",
    "    y_n[i] = np.argmax(ext_1[i][1].cpu().detach().numpy(), axis = 0)\n",
    "x_ts = np.zeros((len(test_ext_1), dim_output))\n",
    "y_n_ts = np.zeros((len(test_ext_1)))\n",
    "for i in range(len(test_ext_1)):\n",
    "    x[i, :] = test_ext_1[i][0].reshape(1, dim_output)\n",
    "    y_n_ts[i] = np.argmax(test_ext_1[i][1].cpu().detach().numpy(), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "model_conv.eval()\n",
    "for (data, target, name) in test_loader:\n",
    "    data = data.cuda()\n",
    "    output = model_conv(data)\n",
    "    output = output.cpu().detach().numpy()\n",
    "    for i, (e, n) in enumerate(list(zip(output, name))):\n",
    "        sub.loc[sub['Image'] == n, 'Id'] = ' '.join(le.inverse_transform(e.argsort()[-5:][::-1]))\n",
    "        \n",
    "sub.to_csv(f'../result/submission_RES18_{time.ctime()}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
